{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11a01432ef74d6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Building GPT from scratch.\n",
    "\n",
    "Building GPT2 is a complex task but I guess I can learn about transformers. For now on, I don't really understand about `attention` and no basics with `RNN`, `LSTM`. It might be a challenging task to understand the transformers of course. But, I have built ViT (Vision Transformer). Why don't I give it a shot?\n",
    "\n",
    "* References\n",
    "\n",
    "  * Illustrated-gpt2 by jay alammar : https://jalammar.github.io/illustrated-gpt2/#part-1-got-and-language-modeling\n",
    "  \n",
    "  * Here's how you can build and train GPT-2 : https://dev.to/amit_kharel_aae65abe2b111/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-345n\n",
    "\n",
    "I've used jay alammar's blog post to understand the architecture and how gpt2 works and bottom blog post is for dataset and preprocessing.\n",
    "\n",
    "I want to make GPT from scratch using 140,000 korean article datasets. I've already tokenized using kkma and KR-Bert. GPT needs MaskedMultiHeadSelfAttention, MultiHeadSelfAttention and MultiLayerPerceptron. And, I know how to make 2 of them.\n",
    "\n",
    "\n",
    "<img src=\"images/transformers.png\" width=\"1000\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40c7bf905c6b6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:11.793878300Z",
     "start_time": "2024-11-19T09:46:09.670843800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version : 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer\n",
    "print(f\"torch version : {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156884648dc23b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:11.832053700Z",
     "start_time": "2024-11-19T09:46:11.813054400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3bd4ada3e114a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:13.040942300Z",
     "start_time": "2024-11-19T09:46:11.819058100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from Artificial_Intelligence.pytorch_modules.pytorch_modules import data_setup, engine\n",
    "    from Artificial_Intelligence.helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/DutchVandaline/Artificial_Intelligence.git\n",
    "    !mv Artificial_Intelligence/pytorch_modules .\n",
    "    !mv Artificial_Intelligence/helper_functions.py . # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from Artificial_Intelligence.pytorch_modules.pytorch_modules import data_setup, engine\n",
    "    from Artificial_Intelligence.helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b079f4b093728f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making Transformer Layers\n",
    "- MSA layer need Mask. Attention Mask is the biggest range and inside, there are `padding mask` and `casual mask`. Mask is for focusing or ignoring the token.\n",
    "  - **Padding Mask** : Removing the padding of tokenizer.\n",
    "  - **Casual Mask** : Shown as Upper Triangular Mask. For Auto-Regression Model making it not see the future mask. Example is like following.\n",
    "  ```\n",
    "  [[0, -inf, -inf, -inf, -inf],\n",
    "   [0,    0, -inf, -inf, -inf],\n",
    "   [0,    0,    0, -inf, -inf],\n",
    "   [0,    0,    0,    0, -inf],\n",
    "   [0,    0,    0,    0,    0]]\n",
    "  ```\n",
    "I've used both `padding mask` and `casual mask` as `attention mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ce5f70ab473845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:13.055943900Z",
     "start_time": "2024-11-19T09:46:13.042943300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not used on GPT\n",
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               embedding_dim:int=768,\n",
    "               num_heads:int=12,\n",
    "               attn_dropout:int=0):\n",
    "\n",
    "    super().__init__()\n",
    "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    self.multihead_attn= nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                      num_heads=num_heads,\n",
    "                      dropout=attn_dropout,\n",
    "                      batch_first=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer_norm(x)\n",
    "    attn_output, _ = self.multihead_attn(query=x,\n",
    "                                    key=x,\n",
    "                                    value=x,\n",
    "                                    need_weights=False)\n",
    "\n",
    "    return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d406557538a6d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:13.079942Z",
     "start_time": "2024-11-19T09:46:13.059944Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "           embedding_dim:int=768,\n",
    "           mlp_size:int=3072,\n",
    "           dropout:float=0.1):\n",
    "    super().__init__()\n",
    "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features=embedding_dim,\n",
    "                  out_features=mlp_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(in_features=mlp_size,\n",
    "                  out_features=embedding_dim),\n",
    "        nn.Dropout(p=dropout))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.mlp(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9058e56e2676ba7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:13.087942100Z",
     "start_time": "2024-11-19T09:46:13.072943300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MaskedMultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int = 768,\n",
    "                 num_heads: int = 12,\n",
    "                 attn_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                   num_heads=num_heads,\n",
    "                                                   dropout=attn_dropout,\n",
    "                                                   batch_first=True)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # Normalize input\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # Masked Self-Attention\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Create causal mask (lower triangular matrix for self-attention)\n",
    "        casual_mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n",
    "\n",
    "        # Apply MultiheadAttention\n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            attn_mask=casual_mask,  # Causal mask for self-attention\n",
    "            key_padding_mask=key_padding_mask,  # Padding mask\n",
    "            need_weights=False\n",
    "        )\n",
    "        \n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdaedda3f38dab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making TransformerDecoderBlock\n",
    "GPT Transformer block doesn't use MSA block. It uses Masked MultiheadSelfAttentionLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb86f53fe108c4fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:13.100942800Z",
     "start_time": "2024-11-19T09:46:13.087942100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int = 768,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_size: int = 3072,\n",
    "                 mlp_dropout: float = 0.1,\n",
    "                 attn_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create Masked Self-Attention block (for autoregressive behavior)\n",
    "        self.masked_msa_block = MaskedMultiHeadSelfAttentionBlock(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        # Create Feed-Forward block (MLP)\n",
    "        self.mlp_block = MLPBlock(\n",
    "            embedding_dim=embedding_dim,\n",
    "            mlp_size=mlp_size,\n",
    "            dropout=mlp_dropout\n",
    "        )\n",
    "\n",
    "        # Layer normalization for each block\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        #print(f\"Before self-attention: {x.isnan().any()}\")\n",
    "\n",
    "        attn_output = self.masked_msa_block(x, key_padding_mask)\n",
    "        x_residual1 = attn_output + x\n",
    "        #print(f\"After self-attention: {x_residual1.isnan().any()}\")\n",
    "        \n",
    "        # Apply Feed-Forward block (MLP) with residual connection\n",
    "        mlp_output = self.mlp_block(x_residual1)\n",
    "        x_residual2 = mlp_output + x_residual1\n",
    "        #print(f\"After feed-forward: {x_residual2.isnan().any()}\")\n",
    "        \n",
    "        return x_residual2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc65993e07ae3b3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:13.114941800Z",
     "start_time": "2024-11-19T09:46:13.101942700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPTDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers: int = 12,\n",
    "                 embedding_dim: int = 768,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_size: int = 3072,\n",
    "                 mlp_dropout: float = 0.1,\n",
    "                 attn_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stack multiple transformer decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(\n",
    "                embedding_dim=embedding_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_size=mlp_size,\n",
    "                mlp_dropout=mlp_dropout,\n",
    "                attn_dropout=attn_dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, key_padding_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7276b921576f4137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:46:13.132987Z",
     "start_time": "2024-11-19T09:46:13.118943100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,  # Vocabulary size\n",
    "                 max_seq_len: int = 256,  # Maximum sequence length\n",
    "                 embedding_dim: int = 768,\n",
    "                 num_layers: int = 12,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_size: int = 3072,\n",
    "                 mlp_dropout: float = 0.1,\n",
    "                 attn_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token Embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Positional Embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, max_seq_len, embedding_dim))\n",
    "\n",
    "        # Decoder stack\n",
    "        self.decoder = GPTDecoder(\n",
    "            num_layers=num_layers,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            mlp_size=mlp_size,\n",
    "            mlp_dropout=mlp_dropout,\n",
    "            attn_dropout=attn_dropout\n",
    "        )\n",
    "\n",
    "        # Output projection to vocab size\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, key_padding_mask=None):\n",
    "        # Step 1: Embed tokens and add positional embeddings\n",
    "        x = self.token_embedding(input_ids)  # Shape: [batch_size, seq_len, embedding_dim]\n",
    "        #print(f\"Token Embedding (x) shape: {x.shape}\")\n",
    "        #print(f\"x contains NaN after token embedding: {x.isnan().any()}\")\n",
    "\n",
    "        seq_len = input_ids.size(1)\n",
    "        x = x + self.positional_embedding[:, :seq_len, :]  # Add positional embedding\n",
    "        #print(f\"x after adding positional embedding: {x.shape}\")\n",
    "        #print(f\"x contains NaN after positional embedding: {x.isnan().any()}\")\n",
    "\n",
    "        # Step 2: Check for NaN in input (if key_padding_mask is provided)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)  # Ensure the mask is of bool type\n",
    "            #print(f\"key_padding_mask contains NaN: {key_padding_mask.isnan().any()}\")\n",
    "\n",
    "        # Step 3: Pass through decoder stack\n",
    "        x = self.decoder(x, key_padding_mask)\n",
    "        #print(f\"x after decoder: {x.shape}\")\n",
    "        #print(f\"x contains NaN after decoder: {x.isnan().any()}\")\n",
    "\n",
    "        # Step 4: Output projection to vocab size\n",
    "        logits = self.output_layer(x)\n",
    "        #print(f\"logits shape: {logits.shape}\")\n",
    "        #print(f\"logits contains NaN: {logits.isnan().any()}\")\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a3cab534f8644",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making Dataloader\n",
    "Data is based on AI-Hub firstly, made of 20,000 Korean Article. Tokenizer was done by kkma and KR-Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08711e5b5d660d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:48:31.779068Z",
     "start_time": "2024-11-19T09:46:14.082752500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256]) torch.Size([8, 256]) torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PreprocessedKoreanDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list = [f for f in os.listdir(data_dir) if f.endswith('.pt')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        data = torch.load(file_path, weights_only=True)\n",
    "\n",
    "        input_ids = data['input_ids']\n",
    "        attention_mask = data['attention_mask']\n",
    "\n",
    "        # Create key_padding_mask based on attention_mask\n",
    "        # Key padding mask is 1 for padded tokens, 0 for non-padded tokens\n",
    "        key_padding_mask = (attention_mask == 0).bool()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'key_padding_mask': key_padding_mask\n",
    "        }\n",
    "\n",
    "#data_dir = \"krbert_korean_pretrain\"\n",
    "data_dir = \"C:/junha/Git_Clone/LLM_Classifier/data/training/training/korean\"\n",
    "batch_size = 16\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = PreprocessedKoreanDataset(data_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    key_padding_mask = batch['key_padding_mask']\n",
    "\n",
    "print(input_ids.shape, attention_mask.shape, key_padding_mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4476b251b60578c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73eb5ffb60e0049a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:48:31.806130700Z",
     "start_time": "2024-11-19T09:47:38.059308200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "krbert_tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-Medium\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "694ff1318e97dabe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:48:31.807130200Z",
     "start_time": "2024-11-19T09:47:38.358511900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs Min: 0, Max: 19887\n",
      "Padding Token ID: 0\n",
      "Input IDs Example: tensor([    2,  8603,  9235,    26,  2304,  3974,  8933,  5014,  2995,  3382,\n",
      "         5207, 16160,  2780, 15015,  4492,  2007,  3758,  2367,  2104,  3678,\n",
      "         2897, 14945,  3504,  2367,  9226,   417,  8823,  3749,  1932,  5364,\n",
      "         5028,  3737,  4059,  2007,  5034,  3585,  9570,  2489,   417, 11810,\n",
      "         4157,  5023,  9558, 11075,  3721,  9041,  9668,  8802,  4492,  3565,\n",
      "         3030,  3529,  2396,    18, 12292, 15015,  3807,  2104,  3504,  2367,\n",
      "        16257,  5361,  9570,  2489,   417,  3940,  5142, 12388,  2099,  3585,\n",
      "        12713,  3721, 10368,  5076,  2007,  4157,  5023,  5056,  3585,  3325,\n",
      "         3721,  3066,  5043,  5809,  3766,  2871,  5071,  5248,  4578,  5093,\n",
      "         3585,  4560,  3898,  3520,  5199,  5719,  2323,  5067,    20,  3742,\n",
      "         9718,  9092,  3742, 13801,  2489,  3579,  2396,    18,  2104,  3504,\n",
      "         2367,  9554, 12292,  8583,  9222,  4560,  3898,  3520,  5199,  5719,\n",
      "         2323,  5067,  2780,  4157,  5023,  4492,  2007,  3289,  9668,  3749,\n",
      "         9208, 12400,  2780,  3806,  3565,  5059,  4492, 14182,  2995,  5134,\n",
      "         5016,  5871,  5034,  2804,    16, 15072,  2367,  9323, 11655,  9702,\n",
      "         4157,  5023,  9877,  8667, 13372,  9668,  9217,  2489,  3565, 10295,\n",
      "         1923,  5399,  4511,  5034,  3579,  2396,  5483,  9223,  4157,  5023,\n",
      "         5473,  3742, 12832,  3242,  5349,  2729,  8833,  3807,  5553,  2489,\n",
      "         2099,  9199,  3585,  8583,  2531,  8846,  9235,  2367,  1998,  5410,\n",
      "         5452,  3742, 10295,  2325,  3514,  3940,   417, 13663,  8972,  3807,\n",
      "         5011,  3579,  2396,    18,  8603,  3382,  5040,  3382,  5207, 12292,\n",
      "         9795,  9364,  3737, 11810,  9795, 18299,  2780, 11432,  9668,  9558,\n",
      "         4492,  2367, 11432, 11810,  4157,  5023,  9558, 11075,  3721,  8780,\n",
      "         5432,  1939,  3382,  5207, 16160, 11562,  3585, 11507,  4492,  3579,\n",
      "         2396,  5072, 10201,  3749, 10713,  3579,  2396,    18, 12292, 11562,\n",
      "         2531,  3742, 10224,  3806,  9668,  4157])\n",
      "Attention Mask Example: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs Min: {input_ids.min()}, Max: {input_ids.max()}\")\n",
    "print(f\"Padding Token ID: {krbert_tokenizer.pad_token_id}\")\n",
    "\n",
    "print(f\"Input IDs Example: {input_ids[1]}\")\n",
    "print(f\"Attention Mask Example: {attention_mask[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81a06f5df3b1cd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:48:31.808129500Z",
     "start_time": "2024-11-19T09:47:38.374509900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "vocab_size = krbert_tokenizer.vocab_size\n",
    "\n",
    "gpt = GPT(vocab_size=vocab_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=krbert_tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr=1e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0296a1af41c0e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:48:31.808129500Z",
     "start_time": "2024-11-19T09:47:38.975509900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        logits = model(input_ids, key_padding_mask=attention_mask)\n",
    "\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = input_ids.view(-1).to(device)  # 예측할 레이블은 input_ids 자체\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=-1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    # 평균 손실과 정확도 계산\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    accuracy = correct_preds / total_preds\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a53ef822c54332ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T04:36:25.768952500Z",
     "start_time": "2024-11-19T10:15:28.782464400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac120f88656e49918b32590331592dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.4650, Accuracy: 0.8632\n",
      "Epoch 2 - Loss: 0.5439, Accuracy: 0.8554\n",
      "Epoch 3 - Loss: 0.6652, Accuracy: 0.8434\n",
      "Epoch 4 - Loss: 0.7967, Accuracy: 0.8282\n",
      "Epoch 5 - Loss: 0.8199, Accuracy: 0.8150\n",
      "Epoch 6 - Loss: 0.7668, Accuracy: 0.8127\n",
      "Epoch 7 - Loss: 0.6971, Accuracy: 0.8165\n",
      "Epoch 8 - Loss: 0.6336, Accuracy: 0.8234\n",
      "Epoch 9 - Loss: 0.5791, Accuracy: 0.8308\n",
      "Epoch 10 - Loss: 0.5327, Accuracy: 0.8380\n",
      "Epoch 11 - Loss: 0.4845, Accuracy: 0.8452\n",
      "Epoch 12 - Loss: 0.4382, Accuracy: 0.8519\n",
      "Epoch 13 - Loss: 0.4025, Accuracy: 0.8570\n",
      "Epoch 14 - Loss: 0.3697, Accuracy: 0.8612\n",
      "Epoch 15 - Loss: 0.3392, Accuracy: 0.8654\n",
      "Epoch 16 - Loss: 0.3171, Accuracy: 0.8687\n",
      "Epoch 17 - Loss: 0.3014, Accuracy: 0.8711\n",
      "Epoch 18 - Loss: 0.2872, Accuracy: 0.8734\n",
      "Epoch 19 - Loss: 0.2730, Accuracy: 0.8757\n",
      "Epoch 20 - Loss: 0.2641, Accuracy: 0.8770\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train_step(\n",
    "        model=gpt,\n",
    "        dataloader=dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43724f723854382",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8078b66025ce8b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T04:36:26.127960200Z",
     "start_time": "2024-11-20T04:36:25.768952500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: C:\\junha\\GPT_from_Scratch\\models\\GPTScratch_300K.pth\n"
     ]
    }
   ],
   "source": [
    "from Artificial_Intelligence.pytorch_modules.pytorch_modules import utils\n",
    "\n",
    "utils.save_model(model=gpt,\n",
    "                 target_dir=\"C:/junha/GPT_from_Scratch/models\",\n",
    "                 model_name=\"GPTScratch_300K.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "114b13ea8bb9c29b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T05:45:50.655624200Z",
     "start_time": "2024-11-20T05:45:50.029625400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPTWithGenerate(GPT):\n",
    "    def generate(self, input_ids, max_length=50, num_return_sequences=1, temperature=1.0, top_k=50):\n",
    "        self.eval()  # 평가 모드로 설정\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "        generated_sequences = []\n",
    "\n",
    "        # 여러 시퀀스를 생성할 경우\n",
    "        for _ in range(num_return_sequences):\n",
    "            generated_ids = input_ids  # 초기 입력 시퀀스\n",
    "            for _ in range(max_length):\n",
    "                # Step 1: 모델의 출력을 얻기 위해 forward 패스 실행\n",
    "                outputs = self(input_ids=generated_ids)\n",
    "                logits = outputs  # 모델의 logits, shape: [batch_size, seq_len, vocab_size]\n",
    "                logits = logits[:, -1, :] / temperature  # 마지막 위치의 logits만 사용하고, temperature로 스케일링\n",
    "                \n",
    "                # Step 2: 확률로 변환 (softmax)\n",
    "                next_token_probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Step 3: top-k 샘플링 (선택적)\n",
    "                if top_k > 0:\n",
    "                    top_k_probs, top_k_indices = torch.topk(next_token_probs, top_k, dim=-1)\n",
    "                    next_token = torch.multinomial(top_k_probs, 1)  # top-k 확률 분포에서 샘플링\n",
    "                    next_token = top_k_indices.gather(-1, next_token)\n",
    "                else:\n",
    "                    next_token = torch.argmax(next_token_probs, dim=-1).unsqueeze(-1)  # argmax로 다음 토큰 예측\n",
    "\n",
    "                # Step 4: 예측된 토큰을 입력 시퀀스에 추가\n",
    "                generated_ids = torch.cat((generated_ids, next_token), dim=-1)\n",
    "\n",
    "            generated_sequences.append(generated_ids)\n",
    "\n",
    "        return generated_sequences\n",
    "\n",
    "# 기존 GPT 모델을 GPTWithGenerate 클래스로 확장하여 사용\n",
    "gptScratch = GPTWithGenerate(vocab_size=vocab_size).to(device)\n",
    "gptScratch.load_state_dict(torch.load(\"C:/junha/GPT_from_Scratch/models/GPTScratch_300K.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37a1925dd5f1137f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T05:46:31.374748400Z",
     "start_time": "2024-11-20T05:46:30.773747600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[    2,  9062,  2244, 10223,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,  9472,  9472,  9472,\n",
      "          9472,  9472,  5031,  5031,  5031]], device='cuda:0')]\n",
      "오늘 날씨가 가격 가격 가격 가격 가격미미미\n"
     ]
    }
   ],
   "source": [
    "prompt = \"오늘 날씨가 \"\n",
    "input_ids = krbert_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "generated_sequences = gptScratch.generate(input_ids, max_length=50, num_return_sequences=1, temperature=1.0, top_k=50)\n",
    "print(generated_sequences)\n",
    "generated_text = krbert_tokenizer.decode(generated_sequences[0][0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786ab3c8cde9e42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model_checkpoint(checkpoint_path, model, optimizer=None):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    print(f\"Last trained epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return epoch, loss\n",
    "\n",
    "vocab_size = krbert_tokenizer.vocab_size\n",
    "gpt = GPT(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "checkpoint_path = \"/home/junha/GPT_from_Scratch/checkpoints/gpt_final_epoch.pt\"\n",
    "\n",
    "start_epoch, last_loss = load_model_checkpoint(checkpoint_path, gpt, optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
